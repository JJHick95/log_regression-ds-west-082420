{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Learning goals\n", "\n", "1. Compare predicting a continuous outcome to predicting a class\n", "2. Compare linear to logistic regression as classification models\n", "3. Describe how logistic regression works under the hood\n", "4. Learn how to interpret a trained logistic model's coefficients\n", "5. Familiarize ourselves with Maximum Likelihood Estimation\n", "6. Explore the C (inverse regularization) paramater and hyperparameter tune\n", "7. Describe the assumptions of logistic regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Why logistic regression as the 1st of our classifiers?\n", "\n", "There are lots of classification algorithms that are available, but the logistics regression is common and is a useful regression method for solving the binary classification problem.\n", "\n", "Logistic regression takes a concept we are familiar with, a linear equation, and translates it into a form fit for predicting a class.  \n", "\n", "It generally can't compete with the best supervised learning algorithms, but it is **simple, fast, and interpretable**.  \n", "\n", "As we will see in mod 4, it will also serve as a segue into our lessons on **neural nets**.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Compare predicting a continuous outcome to predicting a class\n", "\n", "Thus far, we have worked to predict continuous target variables using linear regression. \n", "\n", "  - Continous target variables:\n", "        - Sales price of a home\n", "        - MPG of a car\n", "        - A country's life expectancy rate\n", "        \n", "We will now transition into another category of prediction: classification. Instead of continous target variables, we will be predicting whether records from are data are labeled as a particular class.  Whereas the output for the linear regression model can be any number, the output of our classification algorithms can only be a value designated by a set of discrete outcomes.\n", "\n", "  - Categorical target variables:\n", "        - Whether an employee will stay at a company or leave (churn)\n", "        - Whether a tumor is cancerous or benign\n", "        - Whether a flower is a rose, a dandelion, a tulip, or a daffodil\n", "        - Whether a voter is Republican, Democrat, or Independent\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's navigate to the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets.php?format=&task=cla&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table), and browse there classification datasets.  Which one's catch your eye?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![discuss](https://media.giphy.com/media/l0MYIAUWRmVVzfHag/giphy.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We are still dealing with **labeled data**.\n", "\n", "![labels](https://media.giphy.com/media/26Ff5evMweBsENWqk/giphy.gif)\n", "\n", "\n", "This is still supervised learning. \n", "\n", "But now, instead of the label being a continuous value, such as house price, the label is the category.  This can be either binary or multiclass.  But we still need the **labels** to train our models.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Compare linear to logistic regression as classification models\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The goal of logistic regression, and any classification problem, is to build a model which accurately separates the classes based on independent variables.  \n", "\n", "We are already familiar with how linear regression finds a best-fit \"line\".  It uses the **MSE cost function** to minimize the difference between true and predicted values.  \n", "\n", "A natural thought would be to use that \"line\" to descriminate between classes: Everything with an output greater than a certain point is classified as a 1, everything below is classified as a 0.\n", "\n", "Logistic regression does just this, but in a fancy way. The logistic classifer is **parametric, discriminitive** function.  The best fit parameters ($\\beta$)s creates a decision boundary which allows us to discriminate between the classes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![decision_boundary](https://www.researchgate.net/publication/325813999/figure/fig5/AS:638669773893635@1529282148432/Classification-decision-boundary-using-logistic-regression-The-blue-area-corresponds-to.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Breast Cancer Dataset\n", "\n", "Logistic regression \"is widely used in biostatistical applications where binary responses (two classes) occur quite frequently. For example, patients survive\n", "or die, have heart disease or not, or a condition is present or absent.\"   [Elements of Statistical Learning, Ch. 4, p. 119](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["[data_source](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![breast_cancer_cells](https://storage.googleapis.com/kaggle-datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-card.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have 30 predictor columns, and 1 target column.  Our target column, however, is not in a form suitable for classification.  \n", "\n", "Assumption: **Binary logistic** regression requires the dependent variable to be binary.\n", "\n", "We will define malignant as our positive \"1\" class, and Benign as our \"0\" class."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have a fairly **balanced dataset**.  The logistic regression model will likely be able to pick up on the signal of the minority class.  If it were heavily imbalanced, our model might predict only the majority class, and we would have to use resampling techniques to pick up on the signal."]}, {"cell_type": "markdown", "metadata": {}, "source": ["To begin exploring how logistic regression works, we will fit a linear regression model, and using techniques we know, make predictions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Individual Exercise (turn off camera, take 2 minutes)\n", "\n", "- Use 'area_mean' as the independent variable and 'Target' as dependent variable and apply a linear regression model to this dataset.\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["\n", "lr = LinearRegression()\n", "lr.fit(df[['area_mean']], df[['Target']])\n", "\n", "y_hat = lr.predict(df[['area_mean']])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "- According to the linear regression model, what would be your prediction if area_mean = 350?\n", "\n", "- What about if 'area_mean' is 5?\n", "\n", "- What about 2000?"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[-0.26100989],\n", "       [ 0.07533936],\n", "       [ 1.68396621]])"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["lr.predict(np.array([[5],[350], [2000]]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Those predictions are not within the bounds of our target's sample space. In fact, linear regression could produce predictions from **-$\\infty$ to $\\infty$**  \n", "\n", "\n", "In order to fix that, we can set a threshold which determines a 0 or 1 value.\n", "Let's set the threshhold at .5."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at how many predictions linear regression got wrong."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The confusion matrix will be an important visualization in classification. It will allow us to see the distribution of prediction results. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Volunteer from below to interpret the above CM as type I/II error"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now Let's Try Logistic Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look at that nice S-shape that fits our data so much more naturally."]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Your Turn__\n", "\n", "Use the trained logistic regression and make predictions for \n", "\n", "- area_mean = 350\n", "- area_mean = 5\n", "- area_mean = 2000"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0, 0, 1])"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["log_reg.predict([[5], [350],[2000]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Logistic regression's predict function automatically converts the predicted probabilities to categorical predctions.\n", "To return the probabilities, use the predict_proba method."]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did our logistic regression model compare with our linear regression?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Threshold"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By default, the predict() method applies a threshold of .05 to our prediction probabilities."]}, {"cell_type": "markdown", "metadata": {}, "source": ["However, we may want to be more conservative in our estimate. With medical diagnostics in particular, certain errors are more important to catch.\n", "\n", "Which errors have particularly negative consequences in the scenario above?"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"data": {"text/plain": ["'type II errors, false negatives, are particularly dangerous.  \\nA false negative means a sample was predicted to be benign, when in fact it was malignant'"]}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": ["'''type II errors, false negatives, are particularly dangerous.  \n", "A false negative means a sample was predicted to be benign, when in fact it was malignant'''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To err on the side of caution, we can force our model to predict more conservitavely.  \n", "\n", "By lowering the threshold from .5, our model will predict more positive values, thereby decreasing our false negatives.  Consequently, our false positive rate will go up."]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["yhat_lower_thresh = (log_reg.predict_proba(df[['area_mean']])[:,1] > .4).astype(int)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Logistic Regression Under the Hood"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As we have seen above, linear regression outputs a value that can range anywhere from $-\\infty$ to $\\infty$.  \n", "\n", "Logistic regression attempts to convert those linear outputs to a range of probabilities, i.e. a value between 0 and 1.\n", "\n", "To make this conversion, we use the sigmoid function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![sigmoid](https://media.giphy.com/media/GtKtQ9Gb064uY/giphy.gif)\n", "\n", "\n", "<img src='https://cdn-images-1.medium.com/max/1600/1*RqXFpiNGwdiKBWyLJc_E7g.png' />\n", "\n", "As \u2018Z\u2019 goes to infinity, Y(predicted) will inch closer to 1, and as \u2018Z\u2019 goes to negative infinity, Y(predicted) will inch closer to 0.\n", "\n", "Using the sigmoid function above, if X = 1, the estimated probability would around .7. This tells that there is 80% chance that this observation would fall in the positive class.\n"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["def sigmoid(x):\n", "    \"\"\"\n", "    params: input from linear equation\n", "    returns: probability between 0 and 1\n", "    \"\"\"\n", "    \n", "    return 1/(1+np.e**(-x))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we substitute the product of our linear equation for x in the function above, and rephrase the objective of logistic regression as computing the probability of a class (assume positive class 1) given a set of $\\beta$ parameters, our formula becomes:\n", "\n", "$$\\Large P(Class = 1|X = x) =  \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Some arithmetic (see appendix) allows us to see what the linear equation represents in our logistic regression:\n", "<br><br>\n", "    $\\ln{\\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)}} = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2...\\beta_n*X_n$\n", "    \n", "\n", "Our linear function calculates the log of the probability we predict 1, divided by the probability of predicting 0.  In other words, the linear equation is calculating the **log of the odds** that we predict a class of 1.\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probability and odds\n", "\n", "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n", "\n", "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n", "\n", "Examples:\n", "\n", "- Dice roll of 1: probability = 1/6, odds = 1/5\n", "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n", "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n", "\n", "$$odds = \\frac {probability} {1 - probability}$$\n", "\n", "$$probability = \\frac {odds} {1 + odds}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Odds can assume any positive value, from **0 to $\\infty$**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The log of the odds can take an value from **-$\\infty$ to $\\infty$**, which allows us to map it to the output of the linear equation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check our understanding with the probability predictions of our logistic regression predict_proba output"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Help me out\n", "Reproduce the above log_odds prediction using the coef_ and intercept_ attributes of our fitted log_reg model."]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([-3.43032516])"]}, "execution_count": 50, "metadata": {}, "output_type": "execute_result"}], "source": ["log_reg.coef_[0] * df.area_mean.iloc[3] + log_reg.intercept_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, apply the sigmoid function above to convert the log-odds back to a probability."]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.03136105327730984"]}, "execution_count": 52, "metadata": {}, "output_type": "execute_result"}], "source": ["sigmoid(np.log(odds_sample_4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Interpreting Logistic Regression Coefficients\n", "\n", "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n", "\n", "<img src='img/betas.png' width=700/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Interpretation:** A 1 unit increase in 'area size' is associated with a .0118 unit increase in the log-odds of a malignant result."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Bottom line:** Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Maximum Likelihood Estimation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead of OLS, we will use Maximimum Likelihood Estimation to calculate our $\\beta$s. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["You could use the cost function we used for linear regression, mean-squared error.  However, with a binary outcome, the MSE is **non-convex**.  Gradient descent risks missing the global minimum in favor of a local minimum."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead of optimizing the coefficients based on mean squared error, logistic regression looks to maximize the likelihood of seeing the probabilities given the true class using the following likelihood function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$ \\Large negative\\ loglikelihood = \n", "-\\frac{1}{n} \\sum\\limits_{i=1}^N y_i\\log{p_i} + (1-y_i)\\log(1-p_i) $$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The p variable represents the probabilities of class 1 calculated for each sample, and y represents the true value of the sample.   \n", "\n", "Take a moment to think through how the above Likelihood function rewards coefficients which yield high probabilities of a class matched to the true value."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![log_cost](img/cost_curve_log.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When looking at the above plots of the cost function, we see that as our hypothesis gets closer predicting the correct value, the slope gets much smaller. The effect is \"the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions. The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function.\" [source](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#id4)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike linear regression and its normal equation, there is no closed form solution to minimize the derivative. That is why you may see that non-convergence error.  \n", "\n", "See [here](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) for more detail on MLE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "We have covered how this works for **binary classification problems** (two response classes). But what about **multi-class classification problems** (more than two response classes)?\n", "\n", "- Most common solution for classification models is **\"one-vs-all\"** (also known as **\"one-vs-rest\"**): decompose the problem into multiple binary classification problems\n", "- **Multinomial logistic regression** can solve this as a single problem"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 6. Hyperparameter Tuning the C Variable"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have discussed 'L1' (lasso)  and 'L2' (ridge) regularization.  If you looked at the docstring of Sklearn's Logistic Regression function, you may have noticed that we can specify different types of regularization when fitting the model via the `penalty` parameter.\n", "\n", "We can also specificy the strength of the regularization via the `C` parameter. `C` is the inverse regularization strength.  So, a low `C` means high regularization strength."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's run through our train test split process, and tune our C parameter.   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scaling is important when implementing regularization, since it penalizes the magnitude of the coefficients.\n", "\n", "To correctly implement scaling, we scale only on the training data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair Annotation\n", "\n", "With a partner, put annotations after the empty # comments in the KFold implementation below."]}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": ["\n", "from sklearn.model_selection import KFold \n", "from sklearn.preprocessing import StandardScaler\n", "\n", "c_recall = {}\n", "\n", "# Instantiate a Kfolds instance which we will use to split training into 4 parts\n", "kf = KFold(n_splits=4)\n", "\n", "# Cycle through a series of 100 numbers equally spaced beteen 1 and 1000\n", "# To be used as the C parameter in the logistic regression models fit below\n", "for c in np.linspace(1,1000,100):\n", "    \n", "    mean_recall = []\n", "    \n", "    # Loop through the KFold splits (total of 4 loops), each time reserving a different\n", "    # quarter of the training data as the validation data\n", "    for train_ind, val_ind in kf.split(X_train, y_train):\n", "        \n", "        X_tt, y_tt = X_train.iloc[train_ind], y_train.iloc[train_ind]\n", "        X_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind]\n", "        \n", "        ss = StandardScaler()\n", "        \n", "        \n", "        # Fit the scaler to the training data for each loop.\n", "        # Then transform the validation set.\n", "        # This will prevent data leakage.\n", "        \n", "        X_tt = ss.fit_transform(X_tt)\n", "        X_val = ss.transform(X_val)\n", "        \n", "        # Fit the logistic regression with the c candidate value\n", "        # linked to the current step in the loop.\n", "        log_reg = LogisticRegression(C=c, solver='lbfgs', max_iter=400)\n", "        \n", "        log_reg.fit(X_tt, y_tt)\n", "        \n", "        mean_recall.append(log_reg.score(X_val, y_val))\n", "    \n", "    # Calculate the mean recall score of all validation sets (4 in total)\n", "    # related to the c value of the previous loop\n", "    # and add to the dictionary outside of the loop to store the progress.\n", "    c_recall[c] = np.mean(mean_recall)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Now that we have selected a C hyperparameter that performs well, fit to the entire training set."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can adjust the threshold to catch more false positives."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now apply to the test set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our logistic regression coefficients, we can inspect which features our model thinks are most important for the different classifications."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 8. Assumptions of Logistic Regression\n", "\n", "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms \u2013 particularly regarding linearity, normality, and homoscedasticity.\n", "\n", "First, logistic regression does not require a linear relationship between the dependent and independent variables.  Second, the error terms (residuals) do not need to be normally distributed.  Third, homoscedasticity is not required.  \n", "\n", "**The following assumptions still apply:**\n", "\n", "1.  Binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n", "\n", "2. Logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n", "\n", "3. Logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n", "\n", "4. Logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n", "\n", "5. Logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Appendix: Kfolds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Appendix: Converting sigmoid to log-odds."]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we substitute the product of our linear equation for x in the function above, and rephrase the objective of logistic regression as computing the probability of a class (assume positive class 1) given a set of $\\beta$ parameters, our formula becomes:\n", "\n", "$$\\Large P(Class = 1|X = x) =  \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, with some arithmetic:\n", "\n", "You can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}$\n", "\n", "\n", "$$ \\Large P(G = 1|X = x) = \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n", "\n", "As a result, you can compute:\n", "\n", "$$ \\Large P(G = 0|X =x) = 1- \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Further:\n", "\n", "$$ \\Large \\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)} = e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n} $$\n", "\n", "This expression can be interpreted as the *odds in favor of class 1*.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probability and odds\n", "\n", "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n", "\n", "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n", "\n", "Examples:\n", "\n", "- Dice roll of 1: probability = 1/6, odds = 1/5\n", "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n", "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n", "\n", "$$odds = \\frac {probability} {1 - probability}$$\n", "\n", "$$probability = \\frac {odds} {1 + odds}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This expression can be interpreted as the *odds in favor of class 1*.  \n", "\n", "$$ \\Large \\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)} = e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, taking the log of both sides leads to:\n", "<br><br>\n", "    $\\ln{\\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)}} = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2...\\beta_n*X_n$\n", "    \n", "Here me can see why we call it logisitic regression.\n", "\n", "Our linear function calculates the log of the probability we predict 1, divided by the probability of predicting 0.  In other words, the linear equation is calculating the **log of the odds** that we predict a class of 1.\n", "    "]}], "metadata": {"kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}